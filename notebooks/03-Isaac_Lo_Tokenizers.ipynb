{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook I will explore different tokenizers. The tokenizers that I will create are:\n",
    "\n",
    "- word tokenizer\n",
    "- wordpiece tokenizer\n",
    "- byte pair tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_parquet('../data/processed/sm/train_sm.gzip')\n",
    "test = pd.read_parquet('../data/processed/sm/test_sm.gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 666666 entries, 0 to 666665\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   tweet        666666 non-null  object\n",
      " 1   context      666666 non-null  object\n",
      " 2   label        666666 non-null  int64 \n",
      " 3   tweet_clean  666666 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 20.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tweet          0\ncontext        0\nlabel          0\ntweet_clean    0\ndtype: int64"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               tweet     context  label  \\\n0  just finished downloading fall guys!!! gonna p...  Video Game      3   \n1  Good thread on kink &amp; pride https://t.co/m...     Holiday      5   \n2  $200 | 2.8 JT ‚Ä¢ 24 hrs ‚Ä¢ rt\\n\\n‚û°Ô∏è $100 - follo...  Technology     28   \n\n                                         tweet_clean  \n0  just finished downloading fall guys!!! gonna p...  \n1                    good thread on kink &amp; pride  \n2  $200 | 2.8 jt ‚Ä¢ 24 hrs ‚Ä¢ rt ‚û°Ô∏è $100 - follow ‚ú≥...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>context</th>\n      <th>label</th>\n      <th>tweet_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>just finished downloading fall guys!!! gonna p...</td>\n      <td>Video Game</td>\n      <td>3</td>\n      <td>just finished downloading fall guys!!! gonna p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good thread on kink &amp;amp; pride https://t.co/m...</td>\n      <td>Holiday</td>\n      <td>5</td>\n      <td>good thread on kink &amp;amp; pride</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>$200 | 2.8 JT ‚Ä¢ 24 hrs ‚Ä¢ rt\\n\\n‚û°Ô∏è $100 - follo...</td>\n      <td>Technology</td>\n      <td>28</td>\n      <td>$200 | 2.8 jt ‚Ä¢ 24 hrs ‚Ä¢ rt ‚û°Ô∏è $100 - follow ‚ú≥...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "nltk has a tweet tokenizer that I will use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['just',\n 'finished',\n 'downloading',\n 'fall',\n 'guys',\n '!',\n '!',\n '!',\n 'gonna',\n 'practice',\n 'a',\n 'little',\n 'before',\n 'stream',\n 'sniping',\n 'tf',\n 'out',\n 'of',\n 'my',\n 'faves',\n 'tonight',\n 'üòà',\n 'i',\n 'cant',\n 'wait',\n 'mwahahahaha']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer.tokenize(train['tweet_clean'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "'just finished downloading fall guys!!! gonna practice a little before stream sniping tf out of my faves tonight üòà i cant wait mwahahahaha'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet_clean'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply tokenizer to all clean tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [just, finished, downloading, fall, guys, !, !...\n1                   [good, thread, on, kink, &, pride]\n2    [$, 200, |, 2.8, jt, ‚Ä¢, 24, hrs, ‚Ä¢, rt, ‚û°, Ô∏è, ...\n3    [i, cant, wait, to, hear, yettocome, by, bts, ...\n4             [123, days, ., bring, brittney, home, .]\nName: tweet_clean, dtype: object"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized = train['tweet_clean'].apply(tweet_tokenizer.tokenize)\n",
    "train_tokenized[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "train['tokens'] = train_tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "test_tokenized = test['tweet_clean'].apply(tweet_tokenizer.tokenize)\n",
    "test['tokens'] = test_tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['just', 'finished', 'downloading', ..., 'forget', 'that', '?'],\n      dtype='<U280')"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.hstack(train['tokens'].values)\n",
    "words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[('.', 500875),\n ('the', 430423),\n (',', 325667),\n ('to', 300228),\n ('and', 221122),\n ('a', 212228),\n ('of', 191563),\n ('!', 178139),\n ('in', 173671),\n ('is', 156879)]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = Counter(words)\n",
    "word_counts.most_common(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "word_vocabulary = {'<PAD>': 0, '<UNK>': 1}\n",
    "i = 2\n",
    "for word, count in word_counts.items():\n",
    "    if count > 10:\n",
    "        word_vocabulary[word] = i\n",
    "        i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "39871"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vocabulary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save word vocabulary as json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "with open('../data/processed/vocabularies/word_token_vocab.json', 'w') as f:\n",
    "    f.write(json.dumps(word_vocabulary))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create datasets for train and test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        tokens = row['tokens']\n",
    "\n",
    "        # Front paddings\n",
    "        X = torch.zeros(self.max_len)\n",
    "        for i, token in enumerate(tokens):\n",
    "            X[self.max_len - len(tokens) + i] = self.vocab.get(token, 1)\n",
    "\n",
    "        y = torch.tensor(row['label']).float()\n",
    "\n",
    "        return X.long(), y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(tokens) for tokens in train['tokens']])\n",
    "print(max_len)\n",
    "train_ds = TweetDataset(train, word_vocabulary, max_len)\n",
    "test_ds = TweetDataset(test, word_vocabulary, max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "torch.save(train_ds, '../data/processed/sm/datasets/train_wdtk_sm_ds.pt')\n",
    "torch.save(test_ds, '../data/processed/sm/datasets/test_wdtk_sm_ds.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Byte Subword Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}